{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"This is a sentence. This is another sentence. Woo hoo!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoks = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a sentence.', 'This is another sentence.', 'Woo hoo!']"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EXCERCISE 1: use the word_tokenizer to split the text into word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtoks = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'another',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Woo',\n",
       " 'hoo',\n",
       " '!']"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove stopwords from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"This is a very normal sentence but it has a lot of stopwords in it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal', 'sentence', 'lot', 'stopwords', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in toks if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [\"apples\", \"animals\", \"animation\", \"several\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appl', 'anim', 'anim', 'sever']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EXCERCISE 2: There's another stemmer called the LancasterStemmer. Try it out and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'animal', 'animation', 'several']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lem.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('another', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Help', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(wtoks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A reusable pre-processing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Excercise 3: In the next examples, we're going to be doing a lot of initial preprocessing.\n",
    "### Write a reusable function called \"preprocess\" that takes a document and returns a list of tokenized sentences\n",
    "### Hint. First split it into sentences, then each sentence into words and then tag everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocess a document for NLP.\n",
    "### Takes a plain text document as input\n",
    "### Returns a list of a list of POS-tagged tuples\n",
    "\n",
    "def preprocess(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun-phrase chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Software Development is such an important industry it is hard to imagine a world without it.\n",
    "        Coal mining is also important, but it is usually a more dangerous job than software engineering.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### user-defined regular expression to chunk noun-phrases\n",
    "\n",
    "grammar = (\"NP: {\"           ### label: every noun-phrase that we detect will get this label\n",
    "           \"<DT>?\"           ### zero or one determiners, \n",
    "           \"<JJ>*\"           ### followed by an optional number of adjectives, \n",
    "           \"<NN.*>+\"         ### followed by any type or number of nouns\n",
    "           \"}\")\n",
    "                                        \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (helpful app to develop regex grammars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.app.chunkparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NP chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### takes a pre-processed text, and a grammar (string) and return a result with the noun phrases \"chunked\"\n",
    "def np_chunker(text, grammar):\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    pp_text = preprocess(text)\n",
    "    result = []\n",
    "    for sent in pp_text:\n",
    "        result.append(parser.parse(sent))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunked = np_chunker(text, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('Software', 'NNP'), ('Development', 'NNP')]), ('is', 'VBZ'), ('such', 'JJ'), Tree('NP', [('an', 'DT'), ('important', 'JJ'), ('industry', 'NN')]), ('it', 'PRP'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('imagine', 'VB'), Tree('NP', [('a', 'DT'), ('world', 'NN')]), ('without', 'IN'), ('it', 'PRP'), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('Coal', 'NN'), ('mining', 'NN')]), ('is', 'VBZ'), ('also', 'RB'), ('important', 'JJ'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('usually', 'RB'), ('a', 'DT'), ('more', 'RBR'), Tree('NP', [('dangerous', 'JJ'), ('job', 'NN')]), ('than', 'IN'), Tree('NP', [('software', 'NN'), ('engineering', 'NN')]), ('.', '.')])]"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Software/NNP Development/NNP)\n",
      "(NP an/DT important/JJ industry/NN)\n",
      "(NP a/DT world/NN)\n",
      "(NP Coal/NN mining/NN)\n",
      "(NP dangerous/JJ job/NN)\n",
      "(NP software/NN engineering/NN)\n"
     ]
    }
   ],
   "source": [
    "### now let's go through this result and print out only the noun phrase parts\n",
    "\n",
    "for sentence in chunked:\n",
    "    for n in sentence:\n",
    "        if isinstance(n, nltk.tree.Tree):\n",
    "            if n.label() == 'NP':\n",
    "                print(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### in fact all the hard work has been done for us, with the standard ne_chunk function built into nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"ERNI Consulting AG is a technology consulting company.\n",
    "        The Chief Executive is Daniel Leichti and the Managing Director of Switzerland is Christoph Aeschlimann.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp_text = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (ORGANIZATION ERNI/NNP)\n",
      "  Consulting/NNP\n",
      "  AG/NNP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  technology/NN\n",
      "  consulting/VBG\n",
      "  company/NN\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  Chief/NNP\n",
      "  Executive/NNP\n",
      "  is/VBZ\n",
      "  (PERSON Daniel/NNP Leichti/NNP)\n",
      "  and/CC\n",
      "  the/DT\n",
      "  Managing/NNP\n",
      "  Director/NNP\n",
      "  of/IN\n",
      "  (GPE Switzerland/NNP)\n",
      "  is/VBZ\n",
      "  (PERSON Christoph/NNP Aeschlimann/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for sentence in pp_text:\n",
    "        print(nltk.ne_chunk(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('It', 'PRP'), ('was', 'VBD'), ('built', 'VBN'), ('in', 'IN'), ('honor', 'NN'), ('of', 'IN'), Tree('PERSON', [('George', 'NNP'), ('Washington', 'NNP')]), (',', ','), ('who', 'WP'), ('led', 'VBD'), ('the', 'DT'), ('country', 'NN'), ('to', 'TO'), ('independence', 'VB'), ('and', 'CC'), ('then', 'RB'), ('became', 'VBD'), ('its', 'PRP$'), ('first', 'JJ'), ('President', 'NNP'), ('.', '.')])\n"
     ]
    }
   ],
   "source": [
    "print (entities.__repr__())    ### workaround if you get an error above because ghostscript is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRazor demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An API key for up to 500 free API calls per day is available: https://www.textrazor.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textrazor import TextRazor\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### get the wikipedia article on Donald Trump and use BeautifulSoup to extract only the text\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Donald_Trump'\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "text = BeautifulSoup(html, 'lxml').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### create and initialise the TextRazor client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = '5db1b63a67374cc2a133486d81d5bd0295fce7edd89b9040b777682e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = TextRazor(API_KEY, extractors=[\"topics\", \"categories\", \"relations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.set_classifiers(['textrazor_newscodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = client.analyze(text[8736:16000])      # the main article starts at 8736 chars into the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### assign the topics, categories and relations to variables for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = list(response.topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = list(response.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relations = list(response.relations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump\n",
      "Donald Trump presidential campaign, 2016\n",
      "Politics\n",
      "United States\n",
      "New York Military Academy\n",
      "Politics of the United States\n",
      "Government\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    if topic.score > 0.8:                     ### just the most relevant topics in the text\n",
    "        print (topic.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics>election 0.7668\n",
      "politics 0.7489\n",
      "social issue>family 0.6464\n",
      "politics>politics (general) 0.5866\n",
      "politics>government 0.5528\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    if category.score > 0.5:\n",
    "        print (category.label, category.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = TextRazor(API_KEY, extractors=[\"words\", \"entities\", \"entailments\", \"relations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.set_entity_freebase_type_filters([\"/organization/organization\"])\n",
    "client.set_entity_dbpedia_type_filters([\"Company\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Accenture to acquire OCTO Technology within 6 months.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = client.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relations = response.relations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### takes a list of words and returns the relations in the text\n",
    "def get_relations(wordslist):\n",
    "    relations_list = []\n",
    "    for relation in relations:\n",
    "        for word in relation.predicate_words:\n",
    "            if word.lemma in wordslist:\n",
    "                relations_list.append(relation)\n",
    "                return relations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buy_relations = get_relations([\"sell\", \"buy\", \"acquire\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextRazor Relation at positions [TextRazor Word:\"b'to'\" at position 1, TextRazor Word:\"b'acquire'\" at position 2, TextRazor Word:\"b'within'\" at position 5]]"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found valid sell relationship between:  [TextRazor Entity b'Accenture' at positions [0], TextRazor Entity b'OCTO Technology' at positions [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "for relation in buy_relations:\n",
    "    entity_params = []\n",
    "    for param in relation.params:\n",
    "        all_entities = list(param.entities())\n",
    "\n",
    "        if all_entities:\n",
    "            entity_params.append(all_entities[0])\n",
    "\n",
    "    if len(entity_params) > 1:\n",
    "        print (\"Found valid sell relationship between: \", entity_params)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
