{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"This is a sentence. This is another sentence. Help!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toks = sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a sentence.', 'This is another sentence.', 'Help!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtoks = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'another',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'Help',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove stopwords from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"This is a very normal sentence but it has a lot of stopwords in it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toks = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal', 'sentence', 'lot', 'stopwords', '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in toks if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = [\"apples\", \"animals\", \"animation\", \"several\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['appl', 'anim', 'anim', 'sever']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(w) for w in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'animal', 'animation', 'several']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lem.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('another', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Help', 'NN'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(wtoks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A reusable pre-processing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocess a document for NLP.\n",
    "### Takes a plain text document as input\n",
    "### Returns a list of a list of POS-tagged tuples\n",
    "\n",
    "def preprocess(document):\n",
    "    sentences = sent_tokenize(document)\n",
    "    sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun-phrase chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Software Development is such an important industry it is hard to imagine a world without it. Coal mining is also important, but it is a more dangerous job than software engineering.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_processed = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### user-defined regular expression to chunk noun-phrases\n",
    "\n",
    "grammar = (\"NP: {\"           ### label: every noun-phrase that we detect will get this label\n",
    "           \"<DT>?\"           ### zero or one determiners, \n",
    "           \"<JJ>*\"           ### followed by an optional number of adjectives, \n",
    "           \"<NN.*>+\"         ### followed by any type or number of nouns\n",
    "           \"}\")\n",
    "                                        \n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helpful app to develop grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.app.chunkparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NP chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### takes a pre-processed text, and a grammar (string) and return a result with the noun phrases \"chunked\"\n",
    "def np_chunker(text, grammar):\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    result = []\n",
    "    for sent in post_processed:\n",
    "        result.append(parser.parse(sent))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunked = np_chunker(post_processed, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('NP', [('Software', 'NNP'), ('Development', 'NNP')]), ('is', 'VBZ'), ('such', 'JJ'), Tree('NP', [('an', 'DT'), ('important', 'JJ'), ('industry', 'NN')]), ('it', 'PRP'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('imagine', 'VB'), Tree('NP', [('a', 'DT'), ('world', 'NN')]), ('without', 'IN'), ('it', 'PRP'), ('.', '.')]),\n",
       " Tree('S', [Tree('NP', [('Coal', 'NN'), ('mining', 'NN')]), ('is', 'VBZ'), ('also', 'RB'), ('important', 'JJ'), (',', ','), ('but', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('more', 'RBR'), Tree('NP', [('dangerous', 'JJ'), ('job', 'NN')]), ('than', 'IN'), Tree('NP', [('software', 'NN'), ('engineering', 'NN')]), ('.', '.')])]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Software/NNP Development/NNP)\n",
      "(NP an/DT important/JJ industry/NN)\n",
      "(NP a/DT world/NN)\n",
      "(NP Coal/NN mining/NN)\n",
      "(NP dangerous/JJ job/NN)\n",
      "(NP software/NN engineering/NN)\n"
     ]
    }
   ],
   "source": [
    "### now let's go through this result and print out only the noun phrase parts\n",
    "\n",
    "for sentence in chunked:\n",
    "    for n in sentence:\n",
    "        if isinstance(n, nltk.tree.Tree):\n",
    "            if n.label() == 'NP':\n",
    "                print(n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### in fact all the hard work has been done for us, with the standard ne_chunk function built into nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"The Washington Monument is the most prominent structure in Washington, D.C. and one of the city's early attractions. It was built in honor of George Washington, who led the country to independence and then became its first President.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp_text = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DT'),\n",
       "  ('Washington', 'NNP'),\n",
       "  ('Monument', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('most', 'RBS'),\n",
       "  ('prominent', 'JJ'),\n",
       "  ('structure', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('Washington', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('D.C.', 'NNP'),\n",
       "  ('and', 'CC'),\n",
       "  ('one', 'CD'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('city', 'NN'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('early', 'JJ'),\n",
       "  ('attractions', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('It', 'PRP'),\n",
       "  ('was', 'VBD'),\n",
       "  ('built', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('honor', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('George', 'NNP'),\n",
       "  ('Washington', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('who', 'WP'),\n",
       "  ('led', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('country', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('independence', 'VB'),\n",
       "  ('and', 'CC'),\n",
       "  ('then', 'RB'),\n",
       "  ('became', 'VBD'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('first', 'JJ'),\n",
       "  ('President', 'NNP'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entities = nltk.ne_chunk(pp_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  It/PRP\n",
      "  was/VBD\n",
      "  built/VBN\n",
      "  in/IN\n",
      "  honor/NN\n",
      "  of/IN\n",
      "  (PERSON George/NNP Washington/NNP)\n",
      "  ,/,\n",
      "  who/WP\n",
      "  led/VBD\n",
      "  the/DT\n",
      "  country/NN\n",
      "  to/TO\n",
      "  independence/VB\n",
      "  and/CC\n",
      "  then/RB\n",
      "  became/VBD\n",
      "  its/PRP$\n",
      "  first/JJ\n",
      "  President/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print (entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Washington', 'NNP'), ('Monument', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('most', 'RBS'), ('prominent', 'JJ'), ('structure', 'NN'), ('in', 'IN'), Tree('GPE', [('Washington', 'NNP')]), (',', ','), Tree('GPE', [('D.C.', 'NNP')]), ('and', 'CC'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('city', 'NN'), (\"'s\", 'POS'), ('early', 'JJ'), ('attractions', 'NNS'), ('.', '.')])\n"
     ]
    }
   ],
   "source": [
    "print (entities.__repr__())    ### This is a workaround if you do not have ghostscript installed and gswin64c.exe added to the PATH"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
